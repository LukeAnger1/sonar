{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SONAR: Self-Organizing Network of Aggregated Representations","text":"<p>Project by MIT Media Lab</p> <p>A collaborative learning project where users self-organize to improve their ML models by sharing representations of their data or model. </p> <p>To get started, please refer to the Get Started page.</p>"},{"location":"#main","title":"Main","text":"<p>The application currently uses MPI and GRPC (experimental) to enable communication between different nodes in the network. The goal of the framework to organize everything in a modular manner. That way a researcher or engineer can easily swap out different components of the framework to test their hypothesis or a new algorithm.</p> Topology Train Test 1 2 3 1 2 3 Isolated 208.0<sub>(0.3)</sub> 208.0<sub>(0.3)</sub> 208.0<sub>(0.0)</sub> 44.5<sub>(6.9)</sub> 44.5<sub>(6.9)</sub> 44.5<sub>(6.9)</sub> Central* 208.5<sub>(0.1)</sub> 208.5<sub>(0.0)</sub> 208.5<sub>(0.0)</sub> 33.97<sub>(14.2)</sub> 33.97<sub>(14.2)</sub> 33.97<sub>(14.2)</sub> Random 205.4<sub>(1.0)</sub> 205.5<sub>(0.9)</sub> 205.9<sub>(0.8)</sub> 54.9<sub>(5.3)</sub> 56.0<sub>(5.8)</sub> 56.2<sub>(5.6)</sub> Ring 198.8<sub>(3.1)</sub> 198.7<sub>(3.3)</sub> 198.7<sub>(3.4)</sub> 47.8<sub>(7.3)</sub> 46.9<sub>(6.9)</sub> 47.6<sub>(7.1)</sub> Grid 202.6<sub>(1.5)</sub> 203.9<sub>(1.4)</sub> 204.5<sub>(1.3)</sub> 49.3<sub>(6.0)</sub> 48.8<sub>(6.0)</sub> 48.1<sub>(6.1)</sub> Torus 202.0<sub>(1.2)</sub> 203.2<sub>(1.2)</sub> 204.0<sub>(1.3)</sub> 50.2<sub>(6.0)</sub> 50.7<sub>(6.6)</sub> 50.3<sub>(6.2)</sub> Similarity based (top-k) 206.4<sub>(1.6)</sub> 197.6<sub>(7.3)</sub> 200.4<sub>(4.4)</sub> 47.3<sub>(8.3)</sub> 48.4<sub>(8.5)</sub> 52.8<sub>(7.2)</sub> Swarm 183.2<sub>(3.5)</sub> 183.1<sub>(3.6)</sub> 183.2<sub>(3.6)</sub> 52.2<sub>(8.7)</sub> 52.3<sub>(8.7)</sub> 52.4<sub>(8.6)</sub> L2C 167.0<sub>(25.4)</sub> 158.8<sub>(30.6)</sub> 152.8<sub>(35.2)</sub> 37.6<sub>(7.4)</sub> 36.6<sub>(7.4)</sub> 35.8<sub>(7.7)</sub> <p>Table 1: Performance overview (AUC) of various topologies with different number of collaborators.</p>"},{"location":"algorithms/","title":"Algorithms","text":""},{"location":"algorithms/#main","title":"Main","text":"<p>Here we are comparing the different algorithms deployed.</p> S.No Algorithm Description Paper URL 1 Isolated Isolated algorithm make decisions on the next hop for representations solely based on local information available to the individual node. They don't communicate with other nodes to gather a broader network view. This can lead to suboptimal routing, especially in dynamic or congested networks, as nodes may not be aware of more efficient paths 2 Central Centralized algorithm rely on a central node that possesses complete knowledge of the network topology. This entity calculates the optimal routing paths for all representations and distributes them to the nodes. 3 Random Random algorithm choose the next hop for representations at random. This is a simple approach, but it can lead to very inefficient routing, with representations taking unnecessarily long paths or even getting stuck in loops. 4 Ring In a ring network, representations circulate in a predefined direction (clockwise or counter-clockwise). Each node forwards the packet to its neighbor in that direction. This is a simple and robust approach for dedicated ring topologies. 5 Grid In grid networks, nodes are arranged in a two-dimensional lattice structure. Routing algorithms for grids often employ techniques like XY routing or dimension-order routing, which exploit the grid structure to efficiently determine the next hop towards the destination 6 Torus A torus network is a grid network with \"wrap-around\" connections at the edges. This allows representations to exit from one edge and re-enter from the opposite edge, creating a continuous path. Routing algorithms for torus, leverage similar principles as grid routing while accounting for the wrap-around connections. 7 Similarity based (top-k) Similarity-based (top-k) algorithms utilize similarity measures to compare the data packet's destination with the characteristics of neighboring nodes. Nodes with higher similarity to the destination are more likely to be chosen as the next hop. This can be effective in content-based routing scenarios where representations aim to reach nodes with specific content or properties. 8 Swarm Nodes in the network communicate and share information about their local routing experiences. This collaborative approach can lead to adaptive and efficient routing, particularly in dynamic networks. This is inspired by the collective behavior of biological swarms. 9 L2C L2C algorithms use a model encoder to learn collaboration weights that optimize performance on a local validation set. Nodes dynamically adjust their collaboration strategies based on these learned weights, allowing for more effective peer-to-peer communication. This approach is particularly useful in decentralized networks where data heterogeneity is significant, as it enables nodes to prioritize collaboration with those whose data distributions are more aligned with their own, leading to improved convergence and overall performance."},{"location":"collabench/","title":"CollaBench","text":"<p>Paper: </p>"},{"location":"logging/","title":"Logging Documentation","text":"<p>This document provides a detailed overview of what is being logged in the Sonar setup. </p>"},{"location":"logging/#overview","title":"Overview","text":"<p>This documentation aims to provide transparency on the logging mechanisms implemented in the Sonar project. It includes information on the types of data being logged, their sources, formats, and purposes.</p>"},{"location":"logging/#logging-types","title":"Logging Types","text":"<ul> <li>DEBUG: Detailed information, typically of interest only when diagnosing problems.</li> <li>INFO: Confirmation that things are working as expected.</li> <li>Tensorboard logging: Logging specific metrics, images, and other data to TensorBoard for visualization and analysis.<ul> <li>Console Logging: Logs a message to the console.</li> <li>Scalar Logging: Logs scalar values to TensorBoard for tracking metrics(loss, accuracy) over time.</li> <li>Image Logging: Logs images to both a file and TensorBoard for visual analysis.</li> </ul> </li> </ul>"},{"location":"logging/#log-sources","title":"Log Sources","text":"Component/Module Data Logged Log Level Format Storage Location Frequency/Trigger Model Training (FL) Aggregated model metrics, client updates INFO, DEBUG Plain text <code>./expt_dump/&lt;experiment_name&gt;/logs/client_&lt;client_index&gt;/summary.txt</code> On every FL round"},{"location":"tasks/","title":"Tasks","text":"<p>Welcome to the Tasks section of our repository! Here, you'll find resources and guides for three core tasks in machine learning: Image Classification, Object Detection, and Text Classification.</p>"},{"location":"getting-started/getting-started/","title":"Getting Started","text":"<p>If you would like to contribute to improving this project, please refer to our issues page where we have our open issues in the side navigation bar.</p>"},{"location":"getting-started/getting-started/#installation","title":"Installation","text":"<ul> <li><code>git clone</code> the repository</li> <li><code>pip install -r requirements.txt</code></li> </ul>"},{"location":"getting-started/getting-started/#running-the-code","title":"Running the code","text":"<p>Let's say you want to run the model training of 3 nodes on a machine. That means there will be 4 nodes in total because there is 1 more node in addition to the clients --- server. The whole point of this project is to eventually transition to a distributed system where each node can be a separate machine and a server is simply another node. But for now, this is how things are done. You can do execute the 3 node simulation by running the following command: <code>mpirun -np 4 -host localhost:11 python main.py</code></p>"},{"location":"getting-started/getting-started/#config-file","title":"Config file","text":"<p>The config file is the most important file when running the code. The current set up combines a system config with an algorithm config file. Always be sure of what config you are using. We have intentionally kept configuration files as a python file which is typically a big red flag in software engineering. But we did this because it enables plenty of quick automations and flexibility. Be very careful with the config file because it is easy to overlook some of the configurations such as device ids, number of clients etc.</p>"},{"location":"getting-started/getting-started/#reproducability","title":"Reproducability","text":"<p>One of the awesome things about this project is that whenever you run an experiment, all the source code, logs, and model weights are saved in a separate folder. This is done to ensure that you can reproduce the results by looking at the code that was responsible for the results. The naming of the folder is based on the keys inside the config file. That also means you can not run the same experiment again without renaming/deleting the previous experimental run. The code automatically asks you to press <code>r</code> to remove and create a new folder. Be careful you are not overwriting someone else's results.</p>"},{"location":"getting-started/issues/","title":"Issues","text":"<p>Please view a list of issues to improve this project on the Github Issues Page. </p> <p>There are open tasks for both documentation and project development.</p>"},{"location":"getting-started/issues/documentation/","title":"Documentation","text":"<p>To begin working on the documentation, please follow the set up instructions.</p>"},{"location":"getting-started/issues/documentation/#set-up-mk-docs-on-local","title":"Set up MK Docs on Local","text":"<ul> <li>Clone the repository</li> <li><code>pip install mkdocs-material</code></li> <li><code>mkdocs build</code></li> <li>Add <code>repo_url: https://github.com/aidecentralized/sonar/</code> to <code>mkdocs.yml</code></li> <li><code>mkdocs serve</code></li> </ul>"},{"location":"getting-started/issues/sonar/","title":"SONAR Project","text":"<p>To begin working on the project, please follow the set up instructions.</p>"},{"location":"getting-started/issues/sonar/#set-up-sonar-on-local","title":"Set up SONAR on Local","text":"<ul> <li>Fork this repository (if you would like to build on top of a branch, do not select the option to only fork the main branch)</li> <li>Address an issue and tag the issue number you are addressing in the Pull Request</li> <li>Add one of the leads as a reviewer for the PR</li> </ul>"},{"location":"logging/log_details/","title":"Log Details","text":""},{"location":"logging/log_details/#federated-learning","title":"Federated Learning","text":"<p>Logs aggregated model metrics (loss and accuracy) and updates from clients to track the overall progress and performance of the federated learning process. Additionally, logs include training loss and accuracy from individual clients. Also logs communication events between the server and clients to monitor interactions and data exchange.</p>"},{"location":"logging/logging/","title":"Logging Documentation","text":"<p>This document provides a detailed overview of what is being logged in the Sonar setup. </p>"},{"location":"logging/logging/#overview","title":"Overview","text":"<p>This documentation aims to provide transparency on the logging mechanisms implemented in the Sonar project. It includes information on the types of data being logged, their sources, formats, and purposes.</p>"},{"location":"logging/logging/#logging-types","title":"Logging Types","text":"<ul> <li>DEBUG: Detailed information, typically of interest only when diagnosing problems.</li> <li>INFO: Confirmation that things are working as expected.</li> <li>Tensorboard logging: Logging specific metrics, images, and other data to TensorBoard for visualization and analysis.<ul> <li>Console Logging: Logs a message to the console.</li> <li>Scalar Logging: Logs scalar values to TensorBoard for tracking metrics(loss, accuracy) over time.</li> <li>Image Logging: Logs images to both a file and TensorBoard for visual analysis.</li> </ul> </li> </ul> <p>The tensorboard logs can be viewed by running tensorboard as follows: <code>tensorboard --logdir=expt_dump/ --host 0.0.0.0</code>. Assuming <code>expt_dump</code> is the folder where the experiment logs are stored.</p>"},{"location":"logging/logging/#log-sources","title":"Log Sources","text":"Component/Module Data Logged Log Level Format Storage Location Frequency/Trigger Model Training (FL) Aggregated model metrics, client updates INFO, DEBUG Plain text <code>./expt_dump/&lt;experiment_name&gt;/logs/client_&lt;client_index&gt;/summary.txt</code> On every FL round"},{"location":"tasks/image-classification/","title":"Image Classification","text":""},{"location":"tasks/object-detection/","title":"Object Detection","text":""},{"location":"tasks/object-detection/#overview","title":"Overview","text":"<p>Our environment supports object detection and classification tasks. We support an implementation of YOLOv3 objection detection model using the Pascal VOC dataset. The YOLOv3 (You Only Look Once, version 3) model is a state-of-the-art object detection algorithm known for its speed and accuracy. It performs both object detection and classification in a single forward pass through the network, making it highly efficient. In this project, we adapt YOLOv3 to work in a decentralized machine learning setup, which allows multiple users to train a shared model while keeping their data localized.</p>"},{"location":"tasks/object-detection/#credit","title":"Credit:","text":"<p>The implementation of YOLOv3 in this project is based on the GeeksforGeeks YOLOv3 tutorial. Special thanks to the authors for providing a detailed guide that served as the foundation for this work.</p>"},{"location":"tasks/object-detection/#dataset-preparation","title":"Dataset Preparation","text":"<p>We use the Pascal VOC dataset, a popular benchmark in object detection tasks. Follow the steps below to download and prepare the dataset: 1) Download Pascal VOC Data: * Visit the Pascal VOC Dataset page. * Download the VOC 2012 dataset</p> <p>2) Extract and Structure the Dataset: * Extract the downloaded dataset into a directory of your choice. * Ensure the directory structure is as follows: <pre><code>VOCdevkit/\n  VOC2012/\n    Annotations/\n    ImageSets/\n    JPEGImages/\n    ...\n</code></pre></p> <p>3) (Optional) Split the data * You can split the data according to your desired distribution by labeling a text file with the image names. By default, we will use <code>train.txt</code> and <code>val.txt</code> in <code>VOC2012/Annotations/ImageSets/Main/</code></p>"},{"location":"tasks/object-detection/#configure-the-training","title":"Configure the Training","text":"<p>To set up the training environment, follow these instructions: 1) Install Dependencies: If you haven't already, run <code>pip install -r requirements.txt</code> 2) Configure the system settings. In <code>src/configs/sys_config.py</code>, create a system config object such as the example below, with your desired settings. <pre><code>object_detection_system_config = {\n    \"num_users\": 3, \n    \"dset\": \"pascal\",\n    \"dump_dir\": \"./expt_dump/\", # the path to place the results\n    \"dpath\": \"./datasets/pascal/VOCdevkit/VOC2012/\", # the the location of the dataset\n    # node_0 is a server currently\n    # The device_ids dictionary depicts the GPUs on which the nodes reside.\n    # For a single-GPU environment, the config will look as follows (as it follows a 0-based indexing):\n    \"device_ids\": {\"node_0\": [0], \"node_1\": [0], \"node_2\": [0], \"node_3\": [0]},\n    \"samples_per_user\": 100, \n    \"train_label_distribution\": \"iid\",\n    \"test_label_distribution\": \"iid\",\n    \"folder_deletion_signal_path\":\"./expt_dump/folder_deletion.signal\"\n}\n</code></pre> 3) Configure the algorithm setting. In <code>src/configs/algo_config.py</code>, create an algo config object such as the example below, with your desired algorithm. <pre><code>fedavg_object_detect = {\n    \"algo\": \"fedavg\", # choose any algorithm we support\n    \"exp_id\": \"test\",\n    # Learning setup\n    \"epochs\": 50,\n    \"model\": \"yolo\",\n    \"model_lr\": 1e-5,\n    \"batch_size\": 8,\n}\n</code></pre> 4) Initiate Training: <code>mpirun -n 4 python3 main.py</code> * Note: the <code>-n</code> flag should be followed by (number of desired users+ 1), for the server node * The training will proceed across the users as configured. Monitor printed or saved logs to track progress. * Your result will be written into the <code>dump_dir</code> path specified in <code>sys_config.py</code></p>"},{"location":"tasks/object-detection/#additional-notes","title":"Additional Notes","text":"<ul> <li>Ensure that the setup is correctly configured to avoid issues with client-server communication.</li> <li>If you encounter any issues or have suggestions, please open an issue on our GitHub repository.</li> </ul>"},{"location":"tasks/text-classification/","title":"Text Classification","text":""}]}