{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SONAR: Self-Organizing Network of Aggregated Representations","text":"<p>Project by MIT Media Lab A collaborative learning project where users self-organize to improve their ML models by sharing representations of their data or model.</p>"},{"location":"#main","title":"Main","text":"<p>The application currently uses MPI and GRPC (experimental) to enable communication between different nodes in the network. The goal of the framework to organize everything in a modular manner. That way a researcher or engineer can easily swap out different components of the framework to test their hypothesis or a new algorithm.</p> Topology Train Test 1 2 3 1 2 3 Isolated 208.0<sub>(0.3)</sub> 208.0<sub>(0.3)</sub> 208.0<sub>(0.0)</sub> 44.5<sub>(6.9)</sub> 44.5<sub>(6.9)</sub> 44.5<sub>(6.9)</sub> Central* 208.5<sub>(0.1)</sub> 208.5<sub>(0.0)</sub> 208.5<sub>(0.0)</sub> 33.97<sub>(14.2)</sub> 33.97<sub>(14.2)</sub> 33.97<sub>(14.2)</sub> Random 205.4<sub>(1.0)</sub> 205.5<sub>(0.9)</sub> 205.9<sub>(0.8)</sub> 54.9<sub>(5.3)</sub> 56.0<sub>(5.8)</sub> 56.2<sub>(5.6)</sub> Ring 198.8<sub>(3.1)</sub> 198.7<sub>(3.3)</sub> 198.7<sub>(3.4)</sub> 47.8<sub>(7.3)</sub> 46.9<sub>(6.9)</sub> 47.6<sub>(7.1)</sub> Grid 202.6<sub>(1.5)</sub> 203.9<sub>(1.4)</sub> 204.5<sub>(1.3)</sub> 49.3<sub>(6.0)</sub> 48.8<sub>(6.0)</sub> 48.1<sub>(6.1)</sub> Torus 202.0<sub>(1.2)</sub> 203.2<sub>(1.2)</sub> 204.0<sub>(1.3)</sub> 50.2<sub>(6.0)</sub> 50.7<sub>(6.6)</sub> 50.3<sub>(6.2)</sub> Similarity based (top-k) 206.4<sub>(1.6)</sub> 197.6<sub>(7.3)</sub> 200.4<sub>(4.4)</sub> 47.3<sub>(8.3)</sub> 48.4<sub>(8.5)</sub> 52.8<sub>(7.2)</sub> Swarm 183.2<sub>(3.5)</sub> 183.1<sub>(3.6)</sub> 183.2<sub>(3.6)</sub> 52.2<sub>(8.7)</sub> 52.3<sub>(8.7)</sub> 52.4<sub>(8.6)</sub> L2C 167.0<sub>(25.4)</sub> 158.8<sub>(30.6)</sub> 152.8<sub>(35.2)</sub> 37.6<sub>(7.4)</sub> 36.6<sub>(7.4)</sub> 35.8<sub>(7.7)</sub> <p>Table 1: Performance overview (AUC) of various topologies with different number of collaborators.</p>"},{"location":"#running-the-code","title":"Running the code","text":"<p>Let's say you want to run the model training of 3 nodes on a machine. That means there will be 4 nodes in total because there is 1 more node in addition to the clients --- server. The whole point of this project is to eventually transition to a distributed system where each node can be a separate machine and a server is simply another node. But for now, this is how things are done. You can do execute the 3 node simulation by running the following command: <code>mpirun -np 4 -host localhost:11 python main.py</code></p>"},{"location":"#config-file","title":"Config file","text":"<p>The config file is the most important file when running the code. The current set up combines a system config with an algorithm config file. Always be sure of what config you are using. We have intentionally kept configuration files as a python file which is typically a big red flag in software engineering. But we did this because it enables plenty of quick automations and flexibility. Be very careful with the config file because it is easy to overlook some of the configurations such as device ids, number of clients etc.</p>"},{"location":"#reproducability","title":"Reproducability","text":"<p>One of the awesome things about this project is that whenever you run an experiment, all the source code, logs, and model weights are saved in a separate folder. This is done to ensure that you can reproduce the results by looking at the code that was responsible for the results. The naming of the folder is based on the keys inside the config file. That also means you can not run the same experiment again without renaming/deleting the previous experimental run. The code automatically asks you to press <code>r</code> to remove and create a new folder. Be careful you are not overwriting someone else's results.</p>"},{"location":"#logging","title":"Logging","text":"<p>We log the results in the console and also in a log file that captures the same information. We also log a few metrics for the tensorboard. The tensorboard logs can be viewed by running tensorboard as follows: <code>tensorboard --logdir=expt_dump/ --host 0.0.0.0</code>. Assuming <code>expt_dump</code> is the folder where the experiment logs are stored.</p>"},{"location":"#open-projects-and-tasks","title":"Open Projects and Tasks","text":"<ul> <li>Experiments: Topology Experiments</li> <li>Benchmark Redesign: Improve telemetry, Seperate Topology</li> <li>Comprehensive Documentation</li> </ul>"},{"location":"#set-up-mk-docs-on-local","title":"Set up MK Docs on Local","text":"<ul> <li>Clone the repository</li> <li><code>pip install mkdocs-material</code></li> <li><code>mkdocs build</code></li> <li>Add <code>repo_url: https://github.com/aidecentralized/sonar/</code> to <code>mkdocs.yml</code></li> <li><code>mkdocs serve</code></li> </ul>"},{"location":"algorithms/","title":"Algorithms","text":""},{"location":"algorithms/#main","title":"Main","text":"<p>Here we are comparing the different algorithms deployed.</p> S.No Algorithm Description Paper URL 1 Isolated Isolated algorithm make decisions on the next hop for representations solely based on local information available to the individual node. They don't communicate with other nodes to gather a broader network view. This can lead to suboptimal routing, especially in dynamic or congested networks, as nodes may not be aware of more efficient paths 2 Central Centralized algorithm rely on a central node that possesses complete knowledge of the network topology. This entity calculates the optimal routing paths for all representations and distributes them to the nodes. 3 Random Random algorithm choose the next hop for representations at random. This is a simple approach, but it can lead to very inefficient routing, with representations taking unnecessarily long paths or even getting stuck in loops. 4 Ring In a ring network, representations circulate in a predefined direction (clockwise or counter-clockwise). Each node forwards the packet to its neighbor in that direction. This is a simple and robust approach for dedicated ring topologies. 5 Grid In grid networks, nodes are arranged in a two-dimensional lattice structure. Routing algorithms for grids often employ techniques like XY routing or dimension-order routing, which exploit the grid structure to efficiently determine the next hop towards the destination 6 Torus A torus network is a grid network with \"wrap-around\" connections at the edges. This allows representations to exit from one edge and re-enter from the opposite edge, creating a continuous path. Routing algorithms for torus, leverage similar principles as grid routing while accounting for the wrap-around connections. 7 Similarity based (top-k) Similarity-based (top-k) algorithms utilize similarity measures to compare the data packet's destination with the characteristics of neighboring nodes. Nodes with higher similarity to the destination are more likely to be chosen as the next hop. This can be effective in content-based routing scenarios where representations aim to reach nodes with specific content or properties. 8 Swarm Nodes in the network communicate and share information about their local routing experiences. This collaborative approach can lead to adaptive and efficient routing, particularly in dynamic networks. This is inspired by the collective behavior of biological swarms. 9 L2C L2C algorithms use a model encoder to learn collaboration weights that optimize performance on a local validation set. Nodes dynamically adjust their collaboration strategies based on these learned weights, allowing for more effective peer-to-peer communication. This approach is particularly useful in decentralized networks where data heterogeneity is significant, as it enables nodes to prioritize collaboration with those whose data distributions are more aligned with their own, leading to improved convergence and overall performance."},{"location":"collabench/","title":"Collabench","text":"<p>Paper: </p>"},{"location":"tasks/","title":"Tasks","text":"<p>Welcome to the Tasks section of our repository! Here, you'll find resources and guides for three core tasks in machine learning: Image Classification, Object Detection, and Text Classification.</p>"},{"location":"logging/log_details/","title":"Log Details","text":""},{"location":"logging/log_details/#federated-learning","title":"Federated Learning","text":"<p>Logs aggregated model metrics (loss and accuracy) and updates from clients to track the overall progress and performance of the federated learning process. Additionally, logs include training loss and accuracy from individual clients. Also logs communication events between the server and clients to monitor interactions and data exchange.</p>"},{"location":"logging/logging/","title":"Logging Documentation","text":"<p>This document provides a detailed overview of what is being logged in the Sonar setup. </p>"},{"location":"logging/logging/#overview","title":"Overview","text":"<p>This documentation aims to provide transparency on the logging mechanisms implemented in the Sonar project. It includes information on the types of data being logged, their sources, formats, and purposes.</p>"},{"location":"logging/logging/#logging-types","title":"Logging Types","text":"<ul> <li>DEBUG: Detailed information, typically of interest only when diagnosing problems.</li> <li>INFO: Confirmation that things are working as expected.</li> <li>Tensorboard logging: Logging specific metrics, images, and other data to TensorBoard for visualization and analysis.<ul> <li>Console Logging: Logs a message to the console.</li> <li>Scalar Logging: Logs scalar values to TensorBoard for tracking metrics(loss, accuracy) over time.</li> <li>Image Logging: Logs images to both a file and TensorBoard for visual analysis.</li> </ul> </li> </ul>"},{"location":"logging/logging/#log-sources","title":"Log Sources","text":"Component/Module Data Logged Log Level Format Storage Location Frequency/Trigger Model Training (FL) Aggregated model metrics, client updates INFO, DEBUG Plain text <code>./expt_dump/&lt;experiment_name&gt;/logs/client_&lt;client_index&gt;/summary.txt</code> On every FL round"},{"location":"tasks/image-classification/","title":"Image Classification","text":""},{"location":"tasks/object-detection/","title":"Object Detection","text":""},{"location":"tasks/text-classification/","title":"Text Classification","text":""}]}