{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SONAR: Self-Organizing Network of Aggregated Representations","text":"<p>Project by MIT Media Lab</p> <p>A collaborative learning project where users self-organize to improve their ML models by sharing representations of their data or model. </p> <p>To get started, please refer to the Get Started page.</p>"},{"location":"#main","title":"Main","text":"<p>The application currently uses MPI and GRPC (experimental) to enable communication between different nodes in the network. The goal of the framework to organize everything in a modular manner. That way a researcher or engineer can easily swap out different components of the framework to test their hypothesis or a new algorithm.</p> <p>Table 1: Performance overview (AUC) of various topologies with different number of collaborators.</p> Topology Train Test 1 2 3 1 2 3 Isolated 208.0<sub>(0.3)</sub> 208.0<sub>(0.3)</sub> 208.0<sub>(0.0)</sub> 44.5<sub>(6.9)</sub> 44.5<sub>(6.9)</sub> 44.5<sub>(6.9)</sub> Central* 208.5<sub>(0.1)</sub> 208.5<sub>(0.0)</sub> 208.5<sub>(0.0)</sub> 33.97<sub>(14.2)</sub> 33.97<sub>(14.2)</sub> 33.97<sub>(14.2)</sub> Random 205.4<sub>(1.0)</sub> 205.5<sub>(0.9)</sub> 205.9<sub>(0.8)</sub> 54.9<sub>(5.3)</sub> 56.0<sub>(5.8)</sub> 56.2<sub>(5.6)</sub> Ring 198.8<sub>(3.1)</sub> 198.7<sub>(3.3)</sub> 198.7<sub>(3.4)</sub> 47.8<sub>(7.3)</sub> 46.9<sub>(6.9)</sub> 47.6<sub>(7.1)</sub> Grid 202.6<sub>(1.5)</sub> 203.9<sub>(1.4)</sub> 204.5<sub>(1.3)</sub> 49.3<sub>(6.0)</sub> 48.8<sub>(6.0)</sub> 48.1<sub>(6.1)</sub> Torus 202.0<sub>(1.2)</sub> 203.2<sub>(1.2)</sub> 204.0<sub>(1.3)</sub> 50.2<sub>(6.0)</sub> 50.7<sub>(6.6)</sub> 50.3<sub>(6.2)</sub> Similarity based (top-k) 206.4<sub>(1.6)</sub> 197.6<sub>(7.3)</sub> 200.4<sub>(4.4)</sub> 47.3<sub>(8.3)</sub> 48.4<sub>(8.5)</sub> 52.8<sub>(7.2)</sub> Swarm 183.2<sub>(3.5)</sub> 183.1<sub>(3.6)</sub> 183.2<sub>(3.6)</sub> 52.2<sub>(8.7)</sub> 52.3<sub>(8.7)</sub> 52.4<sub>(8.6)</sub> L2C 167.0<sub>(25.4)</sub> 158.8<sub>(30.6)</sub> 152.8<sub>(35.2)</sub> 37.6<sub>(7.4)</sub> 36.6<sub>(7.4)</sub> 35.8<sub>(7.7)</sub> <p>Table 2 Area Under Curve of Test Accuracy Varying Number of Users</p> Num Users DomainNet (Within Domain) DomainNet (Random) Camelyon17 (Within Domain) Camelyon17 (Random) Digit-Five (Within Domain) Digit-Five (Random) 12 56.6267 50.1772 178.3622 145.6398 57.1168 68.9724 18 59.5647 54.2480 179.5941 145.9916 66.8201 69.8341 24 61.8006 54.3855 178.5976 149.2037 71.6536 72.5333 30 66.5896 58.4835 179.1761 153.0658 74.4239 72.6996 39 68.3743 59.6090 179.1404 149.8618 163.8116 163.9892 45 68.124 59.7852 180.0231 147.4649 77.0248 73.0634 <p>Table 3 Area Under Curve of Test Accuracy Varying Number of Domains</p> <p>AUC DomainNet (48 users, 200 rounds)</p> Num Domains Within Domain Random 2 67.7514 58.7947 4 61.5723 50.2906 6 69.4671 47.7867 <p>AUC Camelyon17 (30 users, 200 rounds)</p> Num Domains Within Domain Random 2 179.7901 172.9167 3 179.1761 153.0658 5 176.5059 139.4547 <p>AUC Digit-Five (30 users, 200 rounds) | Num Domains | Within Domain       | Random        | |-------------|---------------------|---------------| |      2      | 71.8536             | 65.6555       | |      3      | 74.4239             | 72.6996       | |      5      | 77.3709             | 76.3041       |</p> <p>Table 4 Test Accuracy and Standard Deviation Over Rounds</p> <p>DomainNet (39 users, 3 domains) | Rounds | Within Domain |           | Random        |           | |--------|---------------|-----------|---------------|-----------| |        | Mean          | Std       | Mean          | Std       | | 100    | 0.3619        | 0.0635    | 0.3212        | 0.0625    | | 200    | 0.4220        | 0.0563    | 0.3733        | 0.0791    | | 300    | 0.4362        | 0.0498    | 0.4203        | 0.0537    | | 400    | 0.4353        | 0.0687    | 0.4355        | 0.0585    | | 500    | 0.4726        | 0.0502    | 0.4499        | 0.0496    |</p> <p>Camelyon17 (39 users, 3 domains)</p> Rounds Within Domain Random Mean Std Mean Std 40 0.9086 0.0255 0.7281 0.1405 80 0.9169 0.0251 0.7460 0.1196 120 0.9329 0.0195 0.7293 0.1520 160 0.9361 0.0239 0.8122 0.1346 200 0.9353 0.0251 0.7762 0.1516 <p>Digit-Five (39 users, 3 domains)</p> Rounds Within Domain Random Mean Std Mean Std 20 0.7314 0.1290 0.6788 0.0839 40 0.8080 0.0974 0.8151 0.0549 60 0.8350 0.0937 0.8464 0.0558 80 0.8417 0.0928 0.8673 0.0454 100 0.8310 0.1030 0.8733 0.0502"},{"location":"algorithms/","title":"Algorithms","text":""},{"location":"algorithms/#main","title":"Main","text":"<p>Here we are comparing the different algorithms deployed.</p> S.No Algorithm Description Paper URL 1 Isolated Isolated algorithm make decisions on the next hop for representations solely based on local information available to the individual node. They don't communicate with other nodes to gather a broader network view. This can lead to suboptimal routing, especially in dynamic or congested networks, as nodes may not be aware of more efficient paths 2 Central Centralized algorithm rely on a central node that possesses complete knowledge of the network topology. This entity calculates the optimal routing paths for all representations and distributes them to the nodes. 3 Random Random algorithm choose the next hop for representations at random. This is a simple approach, but it can lead to very inefficient routing, with representations taking unnecessarily long paths or even getting stuck in loops. 4 Ring In a ring network, representations circulate in a predefined direction (clockwise or counter-clockwise). Each node forwards the packet to its neighbor in that direction. This is a simple and robust approach for dedicated ring topologies. 5 Grid In grid networks, nodes are arranged in a two-dimensional lattice structure. Routing algorithms for grids often employ techniques like XY routing or dimension-order routing, which exploit the grid structure to efficiently determine the next hop towards the destination 6 Torus A torus network is a grid network with \"wrap-around\" connections at the edges. This allows representations to exit from one edge and re-enter from the opposite edge, creating a continuous path. Routing algorithms for torus, leverage similar principles as grid routing while accounting for the wrap-around connections. 7 Similarity based (top-k) Similarity-based (top-k) algorithms utilize similarity measures to compare the data packet's destination with the characteristics of neighboring nodes. Nodes with higher similarity to the destination are more likely to be chosen as the next hop. This can be effective in content-based routing scenarios where representations aim to reach nodes with specific content or properties. 8 Swarm Nodes in the network communicate and share information about their local routing experiences. This collaborative approach can lead to adaptive and efficient routing, particularly in dynamic networks. This is inspired by the collective behavior of biological swarms. 9 L2C L2C algorithms use a model encoder to learn collaboration weights that optimize performance on a local validation set. Nodes dynamically adjust their collaboration strategies based on these learned weights, allowing for more effective peer-to-peer communication. This approach is particularly useful in decentralized networks where data heterogeneity is significant, as it enables nodes to prioritize collaboration with those whose data distributions are more aligned with their own, leading to improved convergence and overall performance. 10 Meta-L2C This algorithm is a better generalization of L2C that performs meta-learning in order to improve the performance on few-shot collaborator selection problem. 11 Def-KT Decentralized Federated Learning via Mutual Knowledge Transfer is an algorithm that performs knowledge-distillation to improve the performance in a p2p setting. https://arxiv.org/abs/2012.13063 12 FedFomo Personalized Federated Learning with First Order Model Optimization applies an optimization approach to change aggregation weights across nodes to improve the performance of model aggregation. https://arxiv.org/abs/2012.08565"},{"location":"collabench/","title":"CollaBench","text":"<p>Technical Paper: </p>"},{"location":"development/","title":"Development","text":"<p>Please view a list of issues to improve this project on the Github Issues Page. </p> <p>There are open tasks for both documentation and project development.</p>"},{"location":"feature/","title":"Feature Comparison","text":""},{"location":"feature/#comparison-with-existing-fl-frameworks","title":"Comparison with Existing FL Frameworks","text":"<p>There are several frameworks available for federated learning, each with its unique features and capabilities. However, the focus of our work is on decentralized and large scale collaboration. Therefore, we identify the following five key features that are essential for evaluating the capabilities of federated learning frameworks in the context of decentralized collaborative learning:</p> <ol> <li> <p>Decentralized Training: The ability to train models in a decentralized manner, where the training does not necessarily depend upon a central server or a single coordinator.</p> </li> <li> <p>Topology: Support for a wide range of network topologies, including gossip-based, ring, tree, mesh, data-driven, etc. If a framework supports these multiple topologies out of the box, then it becomes easier to rapidly prototype and experiment with different network configurations.</p> </li> <li> <p>Data Partitioning: The ability to handle different data partitioning strategies, such as IID, non-IID, personalized, and multi-task learning. This is crucial for modeling real-world scenarios where data is distributed across multiple users in a non-uniform manner.</p> </li> <li> <p>Privacy &amp; Robustness: Support for privacy-preserving techniques such as differential privacy, secure aggregation, etc., to model privacy concerns in collaborative learning settings. Additionally, a framework should provide testbeds for empirically evaluating the privacy by applying reconstruction or membership inference attacks. From a robustness perspective, the framework should be able to simulate malicious behavior, such as Byzantine attacks, stragglers, and model poisoning etc and provide off-the-shelf support for mechanisms to mitigate these attacks.</p> </li> <li> <p>Collaborator Selection: The ability to select collaborators dynamically based on various criteria such as network conditions, data distribution, model performance, etc. This is essential for large-scale decentralized collaboration where the network topology is dynamic and collaborators may join or leave the network at any time.</p> </li> </ol>"},{"location":"feature/#scoring-mechanism","title":"Scoring Mechanism","text":"<p>We compare the capabilities of existing federated learning frameworks with CollaBench based on the above features. We use a simple scoring mechanism to evaluate the capabilities of each framework. The score ranges from 0 to 2, where 0 indicates no support, 1 indicates partial support, and 2 indicates full support for the feature. Therefore, we get a total score out of 10 for each framework based on the five key features mentioned above.</p> Framework Decentralized training Topology Data Partitioning Privacy &amp; Robustness Collaborator Selection Total Score P2PFL 2 1 0 0 0 3 FedML 2 1 2 1 0 6 EasyFL 0 0 1 0 0 1 COALA 0 0 2 0 0 2 FedScale 1 0 2 0 1 4 Flower 0 0 2 2 0 4 CollaBench (Ours) 1 2 2 0 2 7 <p>Table 1. Head-to-head comparison of capabilities between existing frameworks and CollaBench. Note that this comparison is purely focused on the use-cases of decentralized collaborative learning and does not reflect the overall capabilities of the frameworks. Some of these frameworks are pretty advanced and have additional features that are not relevant to decentralized collaboration.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>If you would like to contribute to improving this project, please refer to our issues page where we have our open issues in the side navigation bar.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<ul> <li><code>git clone</code> the repository</li> <li><code>pip install -r requirements.txt</code></li> </ul>"},{"location":"getting-started/#running-the-code","title":"Running the code","text":"<p>Let's say you want to run the model training of 3 nodes on a machine. That means there will be 4 nodes in total because there is 1 more node in addition to the clients --- server. The whole point of this project is to eventually transition to a distributed system where each node can be a separate machine and a server is simply another node. But for now, this is how things are done. You can do execute the 3 node simulation by running the following command: <code>mpirun -np 4 -host localhost:11 python main.py</code></p>"},{"location":"getting-started/#config-file","title":"Config file","text":"<p>The config file is the most important file when running the code. The current set up combines a system config with an algorithm config file. Always be sure of what config you are using. We have intentionally kept configuration files as a python file which is typically a big red flag in software engineering. But we did this because it enables plenty of quick automations and flexibility. Be very careful with the config file because it is easy to overlook some of the configurations such as device ids, number of clients etc.</p>"},{"location":"getting-started/#reproducability","title":"Reproducability","text":"<p>One of the awesome things about this project is that whenever you run an experiment, all the source code, logs, and model weights are saved in a separate folder. This is done to ensure that you can reproduce the results by looking at the code that was responsible for the results. The naming of the folder is based on the keys inside the config file. That also means you can not run the same experiment again without renaming/deleting the previous experimental run. The code automatically asks you to press <code>r</code> to remove and create a new folder. Be careful you are not overwriting someone else's results.</p>"},{"location":"logging/","title":"Logging Documentation","text":"<p>This document provides a detailed overview of what is being logged in the Sonar setup. </p>"},{"location":"logging/#overview","title":"Overview","text":"<p>This documentation aims to provide transparency on the logging mechanisms implemented in the Sonar project. It includes information on the types of data being logged, their sources, formats, and purposes.</p>"},{"location":"logging/#logging-types","title":"Logging Types","text":"<ul> <li>DEBUG: Detailed information, typically of interest only when diagnosing problems.</li> <li>INFO: Confirmation that things are working as expected.</li> <li>Tensorboard logging: Logging specific metrics, images, and other data to TensorBoard for visualization and analysis.<ul> <li>Console Logging: Logs a message to the console.</li> <li>Scalar Logging: Logs scalar values to TensorBoard for tracking metrics(loss, accuracy) over time.</li> <li>Image Logging: Logs images to both a file and TensorBoard for visual analysis.</li> </ul> </li> </ul> <p>The tensorboard logs can be viewed by running tensorboard as follows: <code>tensorboard --logdir=expt_dump/ --host 0.0.0.0</code>. Assuming <code>expt_dump</code> is the folder where the experiment logs are stored.</p>"},{"location":"logging/#log-sources","title":"Log Sources","text":"Component/Module Data Logged Log Level Format Storage Location Frequency/Trigger Model Training (FL) Aggregated model metrics, client updates INFO, DEBUG Plain text <code>./expt_dump/&lt;experiment_name&gt;/logs/client_&lt;client_index&gt;/summary.txt</code> On every FL round"},{"location":"tasks/","title":"Tasks","text":"<p>Welcome to the Tasks section of our repository! Here, you'll find the core tasks and experiments we have integrated with SONAR. These use cases are being expanded. To contribute to this project, please review our Github Issues page.</p>"},{"location":"datasets/main/","title":"Dataset Description","text":""},{"location":"datasets/main/#image-classification","title":"Image Classification","text":"<p>For most datasets, we use 32x32 image size, with 3 channels. Standard image transformations are applied during both the training and testing phases.  Within each domain, samples are independently and identically distributed (IID) among users belonging to that particular domain. Specifically, users possess 32 training samples for DomainNet and Camelyon17, and 256 samples in the case of Digit-Five.</p>"},{"location":"datasets/main/#domainnet","title":"DomainNet","text":"<p>This dataset contains representations of common objects across six distinct domains sketch, real image, quickdraw, painting, infograph, clipart. From the available 345 classes, we arbitrarily select ten suitcase, teapot, pillow, streetlight, table, bathtub, wine glass, vase, umbrella, bench for the classification task given their substantial sample sizes within each domain. Unless explicitly stated, our experiments are based on the real, sketch and clipart domains.</p>"},{"location":"datasets/main/#camelyon17","title":"Camelyon17","text":"<p>This dataset consists of high-resolution histopathology images of lymph node sections. We use the binary classification task of distinguishing between normal and tumor slides. The dataset is divided into 270 training and 130 testing slides. We treat each hospital as a distinct domain. We further partition samples from each hospital to create a dedicated test set for each domain.</p>"},{"location":"datasets/main/#digit-five","title":"Digit-Five","text":"<p>This dataset comprises images of handwritten digits from five distinct domains MNIST, SVHN, USPS, SYN, and MNIST-M. We use the binary classification task of distinguishing between digits 0 and 1. Each domain contains 256 training samples and 256 testing samples. Common CNN architectures are designed to handle RGB images hence we convert the grayscale images to RGB format by replicating the single channel three times.</p>"},{"location":"datasets/main/#cifar-10","title":"CIFAR-10","text":"<p>This dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. This dataset is commonly used for benchmarking image classification models.</p>"},{"location":"datasets/main/#cifar-100","title":"CIFAR-100","text":"<p>This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).</p>"},{"location":"datasets/main/#medical-mnist","title":"Medical MNIST","text":"<p>This dataset consists of 60,000 28x28 grayscale images of 10 classes (6,000 images per class). The images show different medical conditions, such as pneumonia, emphysema, and cardiomegaly. This dataset is used for lightweight medical image classification tasks. The dataset supports several different biomedical imaging modalities, including X-ray, CT, Histopathology, etc.</p>"},{"location":"datasets/main/#object-detection","title":"Object Detection","text":""},{"location":"datasets/main/#pascal-voc","title":"Pascal-VOC","text":"<p>The PASCAL Visual Object Classes (VOC) dataset is a well-known dataset for object detection. It consists of images annotated with bounding boxes around objects of interest. The dataset contains 20 classes, including animals, vehicles, and household items. The dataset is divided into training and validation sets, with a separate test set for evaluation.</p>"},{"location":"datasets/main/#text-classification","title":"Text Classification","text":""},{"location":"datasets/main/#agnews","title":"AGNews","text":"<p>The AGNews dataset is a widely used benchmark for text classification tasks. It consists of news articles categorized into four distinct classes: World, Sports, Business, and Sci/Tech. Each article is labeled with its corresponding category, making it suitable for training and evaluating text classification models. The dataset is divided into training and test sets, allowing for consistent evaluation of model performance. AGNews is commonly used to benchmark the accuracy and effectiveness of natural language processing algorithms, particularly in the context of news categorization.</p>"},{"location":"datasets/partitioning/","title":"Partitioning Strategy","text":"<p>In decentralized collaborative learning, data partitioning is the process of dividing the data across the participating nodes. Over the last few years, several data partitioning strategies have been proposed to model realistic scenarios and improve the performance of decentralized learning systems. In this section, we discuss some of the data partitioning strategies supported by our framework.</p>"},{"location":"datasets/partitioning/#iid-data-partitioning","title":"IID Data Partitioning","text":"<p>In IID (Independent and Identically Distributed) data partitioning, the data is divided uniformly across the nodes. Each node receives a subset of the data, and the data distribution across the nodes is identical. This partitioning strategy is one of the most simple and widely used strategies when simulating decentralized learning scenarios. However, in real-world scenarios, IID partitioning is rarely observed, as data is often non-IID due to variations in data collection processes, data sources, and data distributions.</p>"},{"location":"datasets/partitioning/#non-iid-data-partitioning","title":"Non-IID Data Partitioning","text":"<p>Non-IID data partitioning refers to a set of strategies where the data distribution across the nodes is non-uniform. In non-IID partitioning, each node receives a distinct subset of the data, and the data distribution across the nodes is heterogeneous. Our framework supports several non-IID partitioning strategies, including:</p>"},{"location":"datasets/partitioning/#label-distribution","title":"Label Distribution","text":"<p>In label distribution partitioning, the data is divided based on the distribution over the labels. We model the non-IID partitioning for labels in two ways - 1. Distinct Labels: Each node receives a distinct set of labels, and 2. Imbalanced Labels: Nodes receive imbalanced label distributions, where some labels are over-represented compared to others. This is modeled using Dirichlet distribution.</p>"},{"location":"datasets/partitioning/#domain-distribution","title":"Domain Distribution","text":"<p>In domain distribution partitioning, the data is divided based on the distribution over the domains. Each node receives a distinct subset of the domains, and the data distribution across the nodes is heterogeneous. This partitioning strategy is useful when the data is collected from multiple sources or domains, and each node represents a distinct domain. For example, in the DomainNet dataset, each node can represent a different domain such as sketch, real image, quickdraw, painting, infograph, or clipart.</p>"},{"location":"datasets/partitioning/#multi-task-learning","title":"Multi-task Learning","text":"<p>In multi-task learning partitioning, the data is divided based on multiple tasks or objectives. Each node receives a distinct subset of tasks, and the data distribution across the nodes is heterogeneous. For example, in Medical MNIST dataset, each node can focus on a specific medical condition such as pneumonia, emphysema, or cardiomegaly.</p>"},{"location":"datasets/partitioning/#feature-distribution","title":"Feature Distribution","text":"<p>In feature distribution partitioning, the data is divided based on the distribution over the features. Each node receives a distinct subset of features, and the data distribution across the nodes is heterogeneous. For example, in CIFAR-10 dataset, we simulate this scenario by rotating the images and dividing them across nodes based on the rotation angle.</p>"},{"location":"development/documentation/","title":"Documentation","text":"<p>To begin working on the documentation, please follow the set up instructions.</p>"},{"location":"development/documentation/#set-up-mk-docs-on-local","title":"Set up MK Docs on Local","text":"<ul> <li>Clone the repository</li> <li><code>pip install mkdocs-material</code></li> <li><code>mkdocs build</code></li> <li>Add <code>repo_url: https://github.com/aidecentralized/sonar/</code> to <code>mkdocs.yml</code></li> <li><code>mkdocs serve</code></li> </ul>"},{"location":"development/sonar/","title":"Contribution Guidelines","text":""},{"location":"development/sonar/#contributing-guidelines","title":"Contributing guidelines","text":"<p>Thank you for your interest in contributing to SONAR! Here are a few pointers about how you can help.</p>"},{"location":"development/sonar/#setting-things-up","title":"Setting things up","text":"<p>To set up the development environment, follow the instructions on Installation Page.</p>"},{"location":"development/sonar/#finding-something-to-work-on","title":"Finding something to work on","text":"<p>The issue tracker of SONAR a good place to start. If you find something that interests you, comment on the thread and we\u2019ll help get you started.</p> <p>If you find a bug, please open an issue in the issue tracker. Please include as much information as possible, including the version you are using, the operating system, and any relevant stack traces or error messages.</p> <p>Alternatively, if you want to add a new feature, please file a new issue and comment if you would like to be assigned. The existing issues are tagged with one or more labels, based on the part of the project it touches, its importance etc., that can help you in selecting one.</p>"},{"location":"development/sonar/#instructions-to-submit-code","title":"Instructions to submit code","text":"<p>Before you submit code, please talk to us via the issue tracker so we know you are working on it.</p> <p>Our central development branch is <code>main</code>. Coding is done on feature branches based off of <code>main</code> and merged into it once stable and reviewed. To submit code, follow these steps:</p> <ol> <li>Create a new branch off of <code>main</code>. Select a descriptive branch name. <pre><code>    git remote add upstream git@github.com:aidecentralized/sonar.git\n    git fetch upstream\n    git checkout main\n    git merge upstream/main\n    git checkout -b your-branch-name\n</code></pre></li> <li>Make your changes and commit them. Make sure to follow the commit message guidelines below.</li> <li>Push your branch to the remote repository.</li> <li>Open a pull request to merge your branch into <code>main</code>.</li> <li>Wait for feedback and review from the team.</li> <li>Once your code is reviewed and approved, the maintainers will merge it into <code>main</code>.</li> <li>Once merged, you can delete your branch.</li> <li> <p>Repeat the process for any additional changes.</p> </li> <li> <p>Commit and push code to your branch:</p> <ul> <li> <p>Commits should be self-contained and contain a descriptive commit message.     ##### Rules for a great git commit message style</p> <ul> <li>Separate subject from body with a blank line</li> <li>Do not end the subject line with a period</li> <li>Capitalize the subject line and each paragraph</li> <li>Use the imperative mood in the subject line</li> <li>Wrap lines at 72 characters</li> <li>Use the body to explain what and why you have done something. In most cases, you can leave out details about how a change has been made.</li> </ul> </li> <li> <p>Please make sure your code is well-formatted and adheres to PEP8 conventions (for Python) and the airbnb style guide (for JavaScript). For others (Lua, prototxt etc.) please ensure that the code is well-formatted and the style consistent.</p> </li> <li>Please ensure that your code is well tested.</li> </ul> </li> <li> <p>Once the code is pushed, create a pull request:</p> <ul> <li>On your GitHub fork, select your branch and click \u201cNew pull request\u201d. Select \u201cmain\u201d as the base branch and your branch in the \u201ccompare\u201d dropdown. If the code is mergeable (you get a message saying \u201cAble to merge\u201d), go ahead and create the pull request.</li> <li>Check back after some time to see if the checks have passed, if not you should click on \u201cDetails\u201d link on your PR thread which will take you to the dashboard for your PR. You will see what failed / stalled, and will need to resolve them.</li> <li> <p>If your checks have passed, your PR will be assigned a reviewer who will review your code and provide comments. Please address each review comment by pushing new commits to the same branch (the PR will automatically update, so you don\u2019t need to submit a new one). Once you are done, comment below each review comment marking it as \u201cDone\u201d. Feel free to use the thread to have a discussion about comments that you don\u2019t understand completely or don\u2019t agree with.</p> </li> <li> <p>Once all comments are addressed, the maintainer will approve the PR.</p> </li> </ul> </li> <li> <p>Once you get reviewed by a mentor and done with all the required changes, squash all the commits: <pre><code>            git checkout &lt;branch_name&gt;\n            git rebase -i HEAD~N (N is the number of commits to be squashed)\n</code></pre></p> <ul> <li>Then a screen will appear with all N commits having \"pick\" written in front of every commit. Change pick to s for the last N-1 commits and let it be pick for the first one.</li> <li> <p>Press esc button and type \":wq\" to save the change and close the screen. Now a new screen will appear asking you to change commit message. Change it accordingly and save it.         git push origin  --force <li> <p>For further query regarding rebasing, visit Squash Commits</p> </li> <li>Once rebasing is done, the reviewer will approve and merge the PR.</li> <p>Congratulations, you have successfully contributed to Project SONAR!</p>"},{"location":"development/sonar/#example-for-a-commit-message","title":"Example for a commit message","text":"<pre><code>Subject of the commit message\n\nBody of the commit message...\n....\n</code></pre>"},{"location":"getting-started/config/","title":"Configuration File Overview","text":""},{"location":"getting-started/config/#importance-of-the-configuration-file","title":"Importance of the Configuration File","text":"<p>The configuration file is the cornerstone of running our codebase, providing the necessary settings to ensure smooth execution. It combines a system configuration with an algorithm configuration to manage both infrastructure and algorithmic parameters. This separation is crucial for maintaining flexibility and clarity, allowing different aspects of the system to be configured independently.</p>"},{"location":"getting-started/config/#why-this-setup","title":"Why This Setup?","text":"<p>We intentionally chose to maintain the configuration as a Python file. The Python-based configuration allows for rapid automation and adaptation, enabling researchers to quickly iterate and customize configurations as needed. However, this flexibility comes with responsibility. It is essential to carefully manage the configuration, as it is easy to overlook critical settings such as device IDs or the number of clients. Always double-check your configuration file before running the code to avoid unintended behavior.</p>"},{"location":"getting-started/config/#purpose-of-configuration-separation","title":"Purpose of Configuration Separation","text":"<p>The configuration files are split into two main parts:</p> <ol> <li> <p>System Configuration (<code>system_config</code>): Manages the infrastructure-related aspects, such as client configurations, GPU device allocation, and data splits. This ensures that researchers focused on algorithm development can work without needing to adjust system-level details.</p> </li> <li> <p>Algorithm Configuration (<code>algo_config</code>): Contains settings specific to the algorithm, such as hyperparameters, learning rates, and model architecture. This configuration is designed to be independent of the system configuration, allowing for modularity and easier experimentation.</p> </li> </ol> <p>These two configurations are combined at runtime in the <code>scheduler.py</code> file, which orchestrates the execution of the code based on the provided settings.</p>"},{"location":"getting-started/config/#example-use-case","title":"Example Use Case","text":"<p>Consider a scenario where you need to allocate specific GPUs to different clients and define unique data splits for each. These details would be managed in the <code>system_config</code>, ensuring that they are isolated from the algorithm's logic. If a researcher wants to test a new optimization algorithm, they can do so by modifying the <code>algo_config</code> without worrying about the underlying system setup.</p> <p>This separation simplifies collaboration and experimentation, allowing different team members to focus on their respective domains without interfering with each other's work.</p>"},{"location":"getting-started/customize/","title":"Customizability","text":"<p>The framework provides an easy way to customize various aspects such as the dataset, model, and topology. By following these steps, users can switch topologies by simply changing a single parameter in the configuration file.</p>"},{"location":"getting-started/customize/#1-customizing-the-dataset","title":"1. Customizing the Dataset:","text":"<p>To customize the dataset, users can modify the data loading code to read data from different sources or apply preprocessing techniques specific to their needs. This can involve changing file paths, data augmentation techniques, or data normalization methods.</p>"},{"location":"getting-started/customize/#example-1","title":"Example 1:","text":"<p>To change the dataset, update the <code>dpath</code> parameter in the <code>sys_config.py</code> file with the address of your dataset folder.</p>"},{"location":"getting-started/customize/#2-customizing-the-model","title":"2. Customizing the Model:","text":"<p>Users can easily customize the model by modifying the type of model you want to use.</p>"},{"location":"getting-started/customize/#example-2","title":"Example 2:","text":"<p>To change the model, update the <code>model</code> parameter in the <code>algo_config.py</code> file. Choose from available options like <code>resnet10</code>, <code>resnet34</code>, <code>yolo</code>, etc.</p>"},{"location":"getting-started/customize/#3-customizing-the-topology","title":"3. Customizing the Topology:","text":"<p>To switch topologies, users can define different topology configurations in the configuration file. By changing a single parameter, the framework will automatically use the corresponding topology during runtime. This allows switching between different network architectures without extensive code modifications. The supported topologies are mentioned in the documentation and new ones are constantly being added.</p>"},{"location":"getting-started/customize/#example-3","title":"Example 3:","text":"<p>In the <code>algo_config.py</code> file, select the desired topology for an experiment by changing the <code>algo</code> parameter. This simple change allows switching between different topologies.</p> <p>By modifying a single parameter in the configuration file, multiple experiments can be run.</p>"},{"location":"getting-started/grpc/","title":"Multi-agent collaboration with gRPC","text":"<p>In this tutorial, we will discuss how to use gRPC for training models across multiple machines. If you have not already read the Getting Started guide, we recommend you do so before proceeding.</p> <p>NOTE:  If you are running experiments on a single machine right now then MPI is easier to set up and get started.</p>"},{"location":"getting-started/grpc/#overview","title":"Overview","text":"<p>The main advantage of our abstract communication layer is that the same code runs regardless of the fact you are using MPI or gRPC underneath. As long as the communication layer is implemented correctly, the rest of the code remains the same. This is a huge advantage for the framework as it allows us to switch between different communication layers without changing the code.</p>"},{"location":"getting-started/grpc/#running-the-code","title":"Running the code","text":"<p>Let's say you want to run the decentralized training with 80 users on 4 machines. Our implementation currently requires a coordinating node to manage the orchestration. Therefore, there will be 81 nodes in total. Make sure <code>sys_config.py</code> has <code>num_users: 80</code> in the config. You should run the following command on all 4 machines:</p> <pre><code>python main_grpc.py -n 20 -host randomhost42.mit.edu\n</code></pre> <p>On one of the machines that you want to use as a coordinator node (let's say it is <code>randomhost43.mit.edu</code>), change the <code>peer_ids</code> with the hostname and the port you want to run the coordinator node and then run the following command:</p> <pre><code>python main.py -super true\n</code></pre> <p>NOTE:  Most of the algorithms right now do not use the new communication protocol, hence you can only use the old MPI version with them. We are working on updating the algorithms to use the new communication protocol.</p>"},{"location":"getting-started/grpc/#protocol","title":"Protocol","text":"<ol> <li>Each node registers to the coordinating node and receives a unique rank.</li> <li>Each node then tries to find available ports to start a listener.</li> <li>Once a quorum has been reached, the coordinating node sends a message to all nodes with the list of all nodes and their ports.</li> <li>Each node then starts local training.</li> </ol>"},{"location":"getting-started/grpc/#faq","title":"FAQ","text":"<ol> <li>Is it really decentralized if it requires a coordinating node?<ul> <li>Yes, it is. The coordinating node is required for all nodes to discover each other. In fact, going forward, we plan to provide support for several nodes to act as coordinating nodes. This will make the system more robust and fault-tolerant. We are looking for contributors to implement a distributed hash table (DHT) to make our system more like BitTorrent. So, if you have a knack for distributed systems, please reach out to us.</li> </ul> </li> <li>Why do I need main_grpc.py and main.py?<ul> <li>The main.py file is the actual file. However, if you are running lots of nodes, it is easier to run the main_grpc.py file which will automatically run the main.py file <code>n</code> times. This is just a convenience script.</li> </ul> </li> <li>How do I know if the nodes are communicating?<ul> <li>You can check the console logs of the coordinating node. It will print out the messages it is sending and receiving. You can also check the logs of the nodes to see their local training status.</li> </ul> </li> <li>My training is stuck or failed. How do I stop and restart?<ul> <li>GRPC simulation starts a lot of threads and even if one of them fail right now then you will have to kill all of them and start all over. So, here is a command to get the pid of all the threads and kill them all at once: <pre><code>for pid in $(ps aux|grep 'python main.py' | cut -b 10-16); do kill -9 $pid; done\n</code></pre> Make sure you remove the experiment folder before starting again.</li> </ul> </li> </ol>"},{"location":"logging/log_details/","title":"Log Details","text":""},{"location":"logging/log_details/#federated-learning","title":"Federated Learning","text":"<p>Logs aggregated model metrics (loss and accuracy) and updates from clients to track the overall progress and performance of the federated learning process. Additionally, logs include training loss and accuracy from individual clients. Also logs communication events between the server and clients to monitor interactions and data exchange.</p>"},{"location":"logging/logging/","title":"Logging Documentation","text":"<p>This document provides a detailed overview of what is being logged in the Sonar setup. </p>"},{"location":"logging/logging/#overview","title":"Overview","text":"<p>This documentation aims to provide transparency on the logging mechanisms implemented in the Sonar project. It includes information on the types of data being logged, their sources, formats, and purposes.</p>"},{"location":"logging/logging/#logging-types","title":"Logging Types","text":"<ul> <li>DEBUG: Detailed information, typically of interest only when diagnosing problems.</li> <li>INFO: Confirmation that things are working as expected.</li> <li>Tensorboard logging: Logging specific metrics, images, and other data to TensorBoard for visualization and analysis.<ul> <li>Console Logging: Logs a message to the console.</li> <li>Scalar Logging: Logs scalar values to TensorBoard for tracking metrics(loss, accuracy) over time.</li> <li>Image Logging: Logs images to both a file and TensorBoard for visual analysis.</li> </ul> </li> </ul> <p>The tensorboard logs can be viewed by running tensorboard as follows: <code>tensorboard --logdir=expt_dump/ --host 0.0.0.0</code>. Assuming <code>expt_dump</code> is the folder where the experiment logs are stored.</p>"},{"location":"logging/logging/#log-sources","title":"Log Sources","text":"Component/Module Data Logged Log Level Format Storage Location Frequency/Trigger Model Training (FL) Aggregated model metrics, client updates INFO, DEBUG Plain text <code>./expt_dump/&lt;experiment_name&gt;/logs/client_&lt;client_index&gt;/summary.txt</code> On every FL round"},{"location":"tasks/image-classification/","title":"Image Classification","text":""},{"location":"tasks/image-classification/#overview","title":"Overview","text":"<p>Our environment supports image classification tasks using various ResNet architectures. We provide implementations of ResNet models tailored for different image classification tasks across several datasets. ResNet (Residual Network) models are widely recognized for their effectiveness in deep learning, particularly in image recognition tasks. In this project, we utilize ResNet models to classify images from multiple datasets, including DomainNet, Camelyon17, Digit-Five, CIFAR-10, CIFAR-100, and Medical MNIST. The implementation is designed to handle decentralized machine learning scenarios, allowing multiple users to train a shared model while keeping their data localized.</p>"},{"location":"tasks/image-classification/#credit","title":"Credit:","text":"<p>Credit to Huawei Technologies Co., Ltd. foss@huawei.com for ResNet. Taken from Huawei ResNet implementation for comparison with DAFL.</p>"},{"location":"tasks/image-classification/#dataset-preparation","title":"Dataset Preparation","text":"<p>We use several datasets for image classification tasks, including DomainNet, Camelyon17, Digit-Five, CIFAR-10, CIFAR-100, and Medical MNIST. Each dataset has specific characteristics and is used for different types of classification tasks. Follow the steps below to download and prepare the datasets: 1) Download the respective datasets from their official sources:    - DomainNet: DomainNet Dataset page    - Camelyon17: Camelyon17 Dataset page    - Digit-Five: Collection of five digit datasets, including MNIST, SVHN, USPS, SYN, MNIST-M    - CIFAR-10 and CIFAR-100: CIFAR Dataset page    - Medical MNIST: Medical MNIST Dataset page</p>"},{"location":"tasks/image-classification/#configure-the-training","title":"Configure the Training","text":"<p>To set up the training environment, follow these instructions: 1) Install Dependencies: If you haven't already, run <code>pip install -r requirements.txt</code>. 2) Configure the system settings. In <code>src/configs/sys_config.py</code>, create a system config object such as the example below, with your desired settings. <pre><code>image_classification_system_config = {\n    \"num_users\": 4, \n    \"dset\": \"cifar10\",\n    \"dump_dir\": \"./expt_dump/\", # the path to place the results\n    \"dpath\": \"./datasets/imgs/cifar10/\", # the location of the dataset\n    \"device_ids\": {\"node_0\": [0], \"node_1\": [0], \"node_2\": [0], \"node_3\": [0]},\n    \"samples_per_user\": 1000, \n    \"train_label_distribution\": \"iid\",\n    \"test_label_distribution\": \"iid\",\n    \"folder_deletion_signal_path\":\"./expt_dump/folder_deletion.signal\"\n}\n</code></pre> 3) Configure the algorithm settings. In <code>src/configs/algo_config.py</code>, create an algo config object such as the example below, with your desired algorithm.</p> <pre><code>fedavg_image_classify = {\n    \"algo\": \"fedavg\", # choose any algorithm we support\n    \"exp_id\": \"image_classification\",\n    # Learning setup\n    \"epochs\": 20,\n    \"model\": \"resnet18\",\n    \"model_lr\": 1e-3,\n    \"batch_size\": 64,\n}\n</code></pre> <p>4) Initiate Training: <code>mpirun -n 5 python3 main.py</code></p> <p>Note: the <code>-n</code> flag should be followed by (number of desired users + 1), for the server node.</p> <p>The training will proceed across the users as configured. Monitor printed or saved logs to track progress.</p> <p>Your result will be written into the <code>dump_dir</code> path specified in <code>sys_config.py</code>.</p>"},{"location":"tasks/image-classification/#additional-notes","title":"Additional Notes","text":"<ul> <li>Ensure that the setup is correctly configured to avoid issues with client-server communication.</li> <li>If you encounter any issues or have suggestions, please open an issue on our GitHub repository.</li> </ul>"},{"location":"tasks/object-detection/","title":"Object Detection","text":""},{"location":"tasks/object-detection/#overview","title":"Overview","text":"<p>Our environment supports object detection and classification tasks. We support an implementation of YOLOv3 objection detection model using the Pascal VOC dataset. The YOLOv3 (You Only Look Once, version 3) model is a state-of-the-art object detection algorithm known for its speed and accuracy. It performs both object detection and classification in a single forward pass through the network, making it highly efficient. In this project, we adapt YOLOv3 to work in a decentralized machine learning setup, which allows multiple users to train a shared model while keeping their data localized.</p>"},{"location":"tasks/object-detection/#credit","title":"Credit","text":"<p>The implementation of YOLOv3 in this project is based on the GeeksforGeeks YOLOv3 tutorial. Special thanks to the authors for providing a detailed guide that served as the foundation for this work.</p>"},{"location":"tasks/object-detection/#dataset-preparation","title":"Dataset Preparation","text":"<p>We use the Pascal VOC dataset, a popular benchmark in object detection tasks. Follow the steps below to download and prepare the dataset: 1) Download Pascal VOC Data:</p> <ul> <li>Visit the Pascal VOC Dataset page.</li> <li>Download the VOC 2012 dataset</li> </ul> <p>2) Extract and Structure the Dataset:</p> <ul> <li>Extract the downloaded dataset into a directory of your choice.</li> <li>Ensure the directory structure is as follows: <pre><code>VOCdevkit/\n  VOC2012/\n    Annotations/\n    ImageSets/\n    JPEGImages/\n    ...\n</code></pre></li> </ul> <p>3) (Optional) Split the data * You can split the data according to your desired distribution by labeling a text file with the image names. By default, we will use <code>train.txt</code> and <code>val.txt</code> in <code>VOC2012/Annotations/ImageSets/Main/</code></p>"},{"location":"tasks/object-detection/#configure-the-training","title":"Configure the Training","text":"<p>To set up the training environment, follow these instructions:</p> <p>1) Install Dependencies: If you haven't already, run <code>pip install -r requirements.txt</code> 2) Configure the system settings. In <code>src/configs/sys_config.py</code>, create a system config object such as the example below, with your desired settings. <pre><code>object_detection_system_config = {\n    \"num_users\": 3, \n    \"dset\": \"pascal\",\n    \"dump_dir\": \"./expt_dump/\", # the path to place the results\n    \"dpath\": \"./datasets/pascal/VOCdevkit/VOC2012/\", # the the location of the dataset\n    # node_0 is a server currently\n    # The device_ids dictionary depicts the GPUs on which the nodes reside.\n    # For a single-GPU environment, the config will look as follows (as it follows a 0-based indexing):\n    \"device_ids\": {\"node_0\": [0], \"node_1\": [0], \"node_2\": [0], \"node_3\": [0]},\n    \"samples_per_user\": 100, \n    \"train_label_distribution\": \"iid\",\n    \"test_label_distribution\": \"iid\",\n    \"folder_deletion_signal_path\":\"./expt_dump/folder_deletion.signal\"\n}\n</code></pre> 3) Configure the algorithm setting. In <code>src/configs/algo_config.py</code>, create an algo config object such as the example below, with your desired algorithm. <pre><code>fedavg_object_detect = {\n    \"algo\": \"fedavg\", # choose any algorithm we support\n    \"exp_id\": \"test\",\n    # Learning setup\n    \"epochs\": 50,\n    \"model\": \"yolo\",\n    \"model_lr\": 1e-5,\n    \"batch_size\": 8,\n}\n</code></pre> 4) Initiate Training: <code>mpirun -n 4 python3 main.py</code></p> <p>Note: the <code>-n</code> flag should be followed by (number of desired users+ 1), for the server node</p> <p>The training will proceed across the users as configured. Monitor printed or saved logs to track progress.</p> <p>Your result will be written into the <code>dump_dir</code> path specified in <code>sys_config.py</code></p>"},{"location":"tasks/object-detection/#additional-notes","title":"Additional Notes","text":"<ul> <li>Ensure that the setup is correctly configured to avoid issues with client-server communication.</li> <li>If you encounter any issues or have suggestions, please open an issue on our GitHub repository.</li> </ul>"},{"location":"tasks/text-classification/","title":"Text Classification","text":"<p>NOTE:  This task is a work in progress. We are currently running experiments to ensure support for Text Classification. It is currently not integrated properly with the system yet.</p>"},{"location":"tasks/text-classification/#overview","title":"Overview","text":"<p>Our environment supports text classification tasks using Long Short-Term Memory (LSTM) networks. We provide an implementation of an LSTM-based text classification model using the AGNews dataset. The LSTM model is a type of recurrent neural network (RNN) that is particularly effective for sequential data such as text. In this project, we adapt an LSTM network to classify news articles into one of four categories: World, Sports, Business, and Sci/Tech. The implementation is designed to handle decentralized machine learning scenarios, where multiple users can train a shared model while keeping their data localized.</p>"},{"location":"tasks/text-classification/#credit","title":"Credit","text":"<p>The AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu). It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015). This dataset is based on the AGNews Dataset. </p>"},{"location":"tasks/text-classification/#dataset-preparation","title":"Dataset Preparation","text":"<p>We use the AGNews dataset, a popular benchmark in text classification tasks. Follow the steps below to download and prepare the dataset: 1) Download AGNews Data:</p> <ul> <li>Visit the HuggingFace AGNews Dataset page to learn more about the dataset. </li> <li>Download the dataset and extract it to your working directory. If you're using HuggingFace, it can be loaded in using the following code: <code>dataset = datasets.load_dataset('ag_news')</code></li> </ul> <p>2) Construct the Vocabulary</p> <p>You must construct the vocabulary and preprocess the data by tokenizing the text and padding sequences to a uniform length. Here is the recommended script for preprocessing. <pre><code># Construct vocabulary from the dataset\nwords = Counter()\n\nfor example in dataset['train']['text']:\n    processed_text = example.lower().translate(str.maketrans('', '', string.punctuation))\n    for word in word_tokenize(processed_text):\n        words[word] += 1\n\nvocab = set(['&lt;unk&gt;', '&lt;bos&gt;', '&lt;eos&gt;', '&lt;pad&gt;'])\ncounter_threshold = 25\n\nfor char, cnt in words.items():\n    if cnt &gt; counter_threshold:\n        vocab.add(char)\n\nword2ind = {char: i for i, char in enumerate(vocab)}\nind2word = {i: char for char, i in word2ind.items()}\n</code></pre></p>"},{"location":"tasks/text-classification/#configure-the-training","title":"Configure the Training","text":"<p>To set up the training environment, follow these instructions: 1) Install Dependencies: If you haven't already, run <code>pip install -r requirements.txt</code>. 2) Configure the system settings. In <code>src/configs/sys_config.py</code>, create a system config object such as the example below, with your desired settings. <pre><code>text_classification_system_config = {\n    \"num_users\": 3, \n    \"dset\": \"agnews\",\n    \"dump_dir\": \"./expt_dump/\", # the path to place the results\n    \"dpath\": \"./datasets/agnews/\", # the location of the dataset\n    \"device_ids\": {\"node_0\": [0], \"node_1\": [0], \"node_2\": [0], \"node_3\": [0]},\n    \"samples_per_user\": 1000, \n    \"train_label_distribution\": \"iid\",\n    \"test_label_distribution\": \"iid\",\n    \"folder_deletion_signal_path\":\"./expt_dump/folder_deletion.signal\"\n}\n</code></pre> 3) Configure the algorithm settings. In <code>src/configs/algo_config.py</code>, create an algo config object such as the example below, with your desired algorithm. <pre><code>fedavg_text_classify = {\n    \"algo\": \"fedavg\", # choose any algorithm we support\n    \"exp_id\": \"test\",\n    # Learning setup\n    \"epochs\": 10,\n    \"model\": \"lstm\",\n    \"model_lr\": 1e-3,\n    \"batch_size\": 64,\n}\n</code></pre> 4) Initiate Training: <code>mpirun -n 4 python3 main.py</code></p> <p>Note: the <code>-n</code> flag should be followed by (number of desired users + 1), for the server node.</p> <p>The training will proceed across the users as configured. Monitor printed or saved logs to track progress.</p> <p>Your result will be written into the <code>dump_dir</code> path specified in <code>sys_config.py</code>.</p>"},{"location":"tasks/text-classification/#additional-notes","title":"Additional Notes","text":"<ul> <li>Ensure that the setup is correctly configured to avoid issues with client-server communication.</li> <li>If you encounter any issues or have suggestions, please open an issue on our GitHub repository.</li> </ul>"}]}