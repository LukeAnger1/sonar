{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SONAR: Self-Organizing Network of Aggregated Representations","text":"<p>Project by MIT Media Lab</p> <p>A collaborative learning project where users self-organize to improve their ML models by sharing representations of their data or model. </p> <p>To get started, please refer to the Get Started page.</p>"},{"location":"#main","title":"Main","text":"<p>The application currently uses MPI and GRPC (experimental) to enable communication between different nodes in the network. The goal of the framework to organize everything in a modular manner. That way a researcher or engineer can easily swap out different components of the framework to test their hypothesis or a new algorithm.</p> Topology Train Test 1 2 3 1 2 3 Isolated 208.0<sub>(0.3)</sub> 208.0<sub>(0.3)</sub> 208.0<sub>(0.0)</sub> 44.5<sub>(6.9)</sub> 44.5<sub>(6.9)</sub> 44.5<sub>(6.9)</sub> Central* 208.5<sub>(0.1)</sub> 208.5<sub>(0.0)</sub> 208.5<sub>(0.0)</sub> 33.97<sub>(14.2)</sub> 33.97<sub>(14.2)</sub> 33.97<sub>(14.2)</sub> Random 205.4<sub>(1.0)</sub> 205.5<sub>(0.9)</sub> 205.9<sub>(0.8)</sub> 54.9<sub>(5.3)</sub> 56.0<sub>(5.8)</sub> 56.2<sub>(5.6)</sub> Ring 198.8<sub>(3.1)</sub> 198.7<sub>(3.3)</sub> 198.7<sub>(3.4)</sub> 47.8<sub>(7.3)</sub> 46.9<sub>(6.9)</sub> 47.6<sub>(7.1)</sub> Grid 202.6<sub>(1.5)</sub> 203.9<sub>(1.4)</sub> 204.5<sub>(1.3)</sub> 49.3<sub>(6.0)</sub> 48.8<sub>(6.0)</sub> 48.1<sub>(6.1)</sub> Torus 202.0<sub>(1.2)</sub> 203.2<sub>(1.2)</sub> 204.0<sub>(1.3)</sub> 50.2<sub>(6.0)</sub> 50.7<sub>(6.6)</sub> 50.3<sub>(6.2)</sub> Similarity based (top-k) 206.4<sub>(1.6)</sub> 197.6<sub>(7.3)</sub> 200.4<sub>(4.4)</sub> 47.3<sub>(8.3)</sub> 48.4<sub>(8.5)</sub> 52.8<sub>(7.2)</sub> Swarm 183.2<sub>(3.5)</sub> 183.1<sub>(3.6)</sub> 183.2<sub>(3.6)</sub> 52.2<sub>(8.7)</sub> 52.3<sub>(8.7)</sub> 52.4<sub>(8.6)</sub> L2C 167.0<sub>(25.4)</sub> 158.8<sub>(30.6)</sub> 152.8<sub>(35.2)</sub> 37.6<sub>(7.4)</sub> 36.6<sub>(7.4)</sub> 35.8<sub>(7.7)</sub> <p>Table 1: Performance overview (AUC) of various topologies with different number of collaborators.</p>"},{"location":"algorithms/","title":"Algorithms","text":""},{"location":"algorithms/#main","title":"Main","text":"<p>Here we are comparing the different algorithms deployed.</p> S.No Algorithm Description Paper URL 1 Isolated Isolated algorithm make decisions on the next hop for representations solely based on local information available to the individual node. They don't communicate with other nodes to gather a broader network view. This can lead to suboptimal routing, especially in dynamic or congested networks, as nodes may not be aware of more efficient paths 2 Central Centralized algorithm rely on a central node that possesses complete knowledge of the network topology. This entity calculates the optimal routing paths for all representations and distributes them to the nodes. 3 Random Random algorithm choose the next hop for representations at random. This is a simple approach, but it can lead to very inefficient routing, with representations taking unnecessarily long paths or even getting stuck in loops. 4 Ring In a ring network, representations circulate in a predefined direction (clockwise or counter-clockwise). Each node forwards the packet to its neighbor in that direction. This is a simple and robust approach for dedicated ring topologies. 5 Grid In grid networks, nodes are arranged in a two-dimensional lattice structure. Routing algorithms for grids often employ techniques like XY routing or dimension-order routing, which exploit the grid structure to efficiently determine the next hop towards the destination 6 Torus A torus network is a grid network with \"wrap-around\" connections at the edges. This allows representations to exit from one edge and re-enter from the opposite edge, creating a continuous path. Routing algorithms for torus, leverage similar principles as grid routing while accounting for the wrap-around connections. 7 Similarity based (top-k) Similarity-based (top-k) algorithms utilize similarity measures to compare the data packet's destination with the characteristics of neighboring nodes. Nodes with higher similarity to the destination are more likely to be chosen as the next hop. This can be effective in content-based routing scenarios where representations aim to reach nodes with specific content or properties. 8 Swarm Nodes in the network communicate and share information about their local routing experiences. This collaborative approach can lead to adaptive and efficient routing, particularly in dynamic networks. This is inspired by the collective behavior of biological swarms. 9 L2C L2C algorithms use a model encoder to learn collaboration weights that optimize performance on a local validation set. Nodes dynamically adjust their collaboration strategies based on these learned weights, allowing for more effective peer-to-peer communication. This approach is particularly useful in decentralized networks where data heterogeneity is significant, as it enables nodes to prioritize collaboration with those whose data distributions are more aligned with their own, leading to improved convergence and overall performance."},{"location":"collabench/","title":"CollaBench","text":"<p>Technical Paper: </p>"},{"location":"logging/","title":"Logging Documentation","text":"<p>This document provides a detailed overview of what is being logged in the Sonar setup. </p>"},{"location":"logging/#overview","title":"Overview","text":"<p>This documentation aims to provide transparency on the logging mechanisms implemented in the Sonar project. It includes information on the types of data being logged, their sources, formats, and purposes.</p>"},{"location":"logging/#logging-types","title":"Logging Types","text":"<ul> <li>DEBUG: Detailed information, typically of interest only when diagnosing problems.</li> <li>INFO: Confirmation that things are working as expected.</li> <li>Tensorboard logging: Logging specific metrics, images, and other data to TensorBoard for visualization and analysis.<ul> <li>Console Logging: Logs a message to the console.</li> <li>Scalar Logging: Logs scalar values to TensorBoard for tracking metrics(loss, accuracy) over time.</li> <li>Image Logging: Logs images to both a file and TensorBoard for visual analysis.</li> </ul> </li> </ul>"},{"location":"logging/#log-sources","title":"Log Sources","text":"Component/Module Data Logged Log Level Format Storage Location Frequency/Trigger Model Training (FL) Aggregated model metrics, client updates INFO, DEBUG Plain text <code>./expt_dump/&lt;experiment_name&gt;/logs/client_&lt;client_index&gt;/summary.txt</code> On every FL round"},{"location":"tasks/","title":"Tasks","text":"<p>Welcome to the Tasks section of our repository! Here, you'll find resources and guides for three core tasks in machine learning: Image Classification, Object Detection, and Text Classification.</p>"},{"location":"datasets/main/","title":"Dataset Description","text":""},{"location":"datasets/main/#image-classification","title":"Image Classification","text":"<p>For most datasets, we use 32x32 image size, with 3 channels. Standard image transformations are applied during both the training and testing phases.  Within each domain, samples are independently and identically distributed (IID) among users belonging to that particular domain. Specifically, users possess 32 training samples for DomainNet and Camelyon17, and 256 samples in the case of Digit-Five.</p>"},{"location":"datasets/main/#domainnet","title":"DomainNet","text":"<p>This dataset contains representations of common objects across six distinct domains sketch, real image, quickdraw, painting, infograph, clipart. From the available 345 classes, we arbitrarily select ten suitcase, teapot, pillow, streetlight, table, bathtub, wine glass, vase, umbrella, bench for the classification task given their substantial sample sizes within each domain. Unless explicitly stated, our experiments are based on the real, sketch and clipart domains.</p>"},{"location":"datasets/main/#camelyon17","title":"Camelyon17","text":"<p>This dataset consists of high-resolution histopathology images of lymph node sections. We use the binary classification task of distinguishing between normal and tumor slides. The dataset is divided into 270 training and 130 testing slides. We treat each hospital as a distinct domain. We further partition samples from each hospital to create a dedicated test set for each domain.</p>"},{"location":"datasets/main/#digit-five","title":"Digit-Five","text":"<p>This dataset comprises images of handwritten digits from five distinct domains MNIST, SVHN, USPS, SYN, and MNIST-M. We use the binary classification task of distinguishing between digits 0 and 1. Each domain contains 256 training samples and 256 testing samples. Common CNN architectures are designed to handle RGB images hence we convert the grayscale images to RGB format by replicating the single channel three times.</p>"},{"location":"datasets/main/#cifar-10","title":"CIFAR-10","text":"<p>This dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. This dataset is commonly used for benchmarking image classification models.</p>"},{"location":"datasets/main/#cifar-100","title":"CIFAR-100","text":"<p>This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).</p>"},{"location":"datasets/main/#medical-mnist","title":"Medical MNIST","text":"<p>This dataset consists of 60,000 28x28 grayscale images of 10 classes (6,000 images per class). The images show different medical conditions, such as pneumonia, emphysema, and cardiomegaly. This dataset is used for lightweight medical image classification tasks. The dataset supports several different biomedical imaging modalities, including X-ray, CT, Histopathology, etc.</p>"},{"location":"datasets/main/#object-detection","title":"Object Detection","text":""},{"location":"datasets/main/#pascal-voc","title":"Pascal-VOC","text":"<p>The PASCAL Visual Object Classes (VOC) dataset is a well-known dataset for object detection. It consists of images annotated with bounding boxes around objects of interest. The dataset contains 20 classes, including animals, vehicles, and household items. The dataset is divided into training and validation sets, with a separate test set for evaluation.</p>"},{"location":"datasets/main/#text-classification","title":"Text Classification","text":""},{"location":"datasets/main/#agnews","title":"AGNews","text":"<p>The AGNews dataset is a widely used benchmark for text classification tasks. It consists of news articles categorized into four distinct classes: World, Sports, Business, and Sci/Tech. Each article is labeled with its corresponding category, making it suitable for training and evaluating text classification models. The dataset is divided into training and test sets, allowing for consistent evaluation of model performance. AGNews is commonly used to benchmark the accuracy and effectiveness of natural language processing algorithms, particularly in the context of news categorization.</p>"},{"location":"datasets/partitioning/","title":"Partitioning Strategy","text":"<p>In decentralized collaborative learning, data partitioning is the process of dividing the data across the participating nodes. Over the last few years, several data partitioning strategies have been proposed to model realistic scenarios and improve the performance of decentralized learning systems. In this section, we discuss some of the data partitioning strategies supported by our framework.</p>"},{"location":"datasets/partitioning/#iid-data-partitioning","title":"IID Data Partitioning","text":"<p>In IID (Independent and Identically Distributed) data partitioning, the data is divided uniformly across the nodes. Each node receives a subset of the data, and the data distribution across the nodes is identical. This partitioning strategy is one of the most simple and widely used strategies when simulating decentralized learning scenarios. However, in real-world scenarios, IID partitioning is rarely observed, as data is often non-IID due to variations in data collection processes, data sources, and data distributions.</p>"},{"location":"datasets/partitioning/#non-iid-data-partitioning","title":"Non-IID Data Partitioning","text":"<p>Non-IID data partitioning refers to a set of strategies where the data distribution across the nodes is non-uniform. In non-IID partitioning, each node receives a distinct subset of the data, and the data distribution across the nodes is heterogeneous. Our framework supports several non-IID partitioning strategies, including:</p>"},{"location":"datasets/partitioning/#label-distribution","title":"Label Distribution","text":"<p>In label distribution partitioning, the data is divided based on the distribution over the labels. We model the non-IID partitioning for labels in two ways - 1. Distinct Labels: Each node receives a distinct set of labels, and 2. Imbalanced Labels: Nodes receive imbalanced label distributions, where some labels are over-represented compared to others. This is modeled using Dirichlet distribution.</p>"},{"location":"datasets/partitioning/#domain-distribution","title":"Domain Distribution","text":"<p>In domain distribution partitioning, the data is divided based on the distribution over the domains. Each node receives a distinct subset of the domains, and the data distribution across the nodes is heterogeneous. This partitioning strategy is useful when the data is collected from multiple sources or domains, and each node represents a distinct domain. For example, in the DomainNet dataset, each node can represent a different domain such as sketch, real image, quickdraw, painting, infograph, or clipart.</p>"},{"location":"datasets/partitioning/#multi-task-learning","title":"Multi-task Learning","text":"<p>In multi-task learning partitioning, the data is divided based on multiple tasks or objectives. Each node receives a distinct subset of tasks, and the data distribution across the nodes is heterogeneous. For example, in Medical MNIST dataset, each node can focus on a specific medical condition such as pneumonia, emphysema, or cardiomegaly.</p>"},{"location":"datasets/partitioning/#feature-distribution","title":"Feature Distribution","text":"<p>In feature distribution partitioning, the data is divided based on the distribution over the features. Each node receives a distinct subset of features, and the data distribution across the nodes is heterogeneous. For example, in CIFAR-10 dataset, we simulate this scenario by rotating the images and dividing them across nodes based on the rotation angle.</p>"},{"location":"getting-started/getting-started/","title":"Getting Started","text":"<p>If you would like to contribute to improving this project, please refer to our issues page where we have our open issues in the side navigation bar.</p>"},{"location":"getting-started/getting-started/#installation","title":"Installation","text":"<ul> <li><code>git clone</code> the repository</li> <li><code>pip install -r requirements.txt</code></li> </ul>"},{"location":"getting-started/getting-started/#running-the-code","title":"Running the code","text":"<p>Let's say you want to run the model training of 3 nodes on a machine. That means there will be 4 nodes in total because there is 1 more node in addition to the clients --- server. The whole point of this project is to eventually transition to a distributed system where each node can be a separate machine and a server is simply another node. But for now, this is how things are done. You can do execute the 3 node simulation by running the following command: <code>mpirun -np 4 -host localhost:11 python main.py</code></p>"},{"location":"getting-started/getting-started/#config-file","title":"Config file","text":"<p>The config file is the most important file when running the code. The current set up combines a system config with an algorithm config file. Always be sure of what config you are using. We have intentionally kept configuration files as a python file which is typically a big red flag in software engineering. But we did this because it enables plenty of quick automations and flexibility. Be very careful with the config file because it is easy to overlook some of the configurations such as device ids, number of clients etc.</p>"},{"location":"getting-started/getting-started/#reproducability","title":"Reproducability","text":"<p>One of the awesome things about this project is that whenever you run an experiment, all the source code, logs, and model weights are saved in a separate folder. This is done to ensure that you can reproduce the results by looking at the code that was responsible for the results. The naming of the folder is based on the keys inside the config file. That also means you can not run the same experiment again without renaming/deleting the previous experimental run. The code automatically asks you to press <code>r</code> to remove and create a new folder. Be careful you are not overwriting someone else's results.</p>"},{"location":"getting-started/issues/","title":"Issues","text":"<p>Please view a list of issues to improve this project on the Github Issues Page. </p> <p>There are open tasks for both documentation and project development.</p>"},{"location":"getting-started/issues/documentation/","title":"Documentation","text":"<p>To begin working on the documentation, please follow the set up instructions.</p>"},{"location":"getting-started/issues/documentation/#set-up-mk-docs-on-local","title":"Set up MK Docs on Local","text":"<ul> <li>Clone the repository</li> <li><code>pip install mkdocs-material</code></li> <li><code>mkdocs build</code></li> <li>Add <code>repo_url: https://github.com/aidecentralized/sonar/</code> to <code>mkdocs.yml</code></li> <li><code>mkdocs serve</code></li> </ul>"},{"location":"getting-started/issues/sonar/","title":"SONAR Project","text":"<p>To begin working on the project, please follow the set up instructions.</p>"},{"location":"getting-started/issues/sonar/#set-up-sonar-on-local","title":"Set up SONAR on Local","text":"<ul> <li>Fork this repository (if you would like to build on top of a branch, do not select the option to only fork the main branch)</li> <li>Address an issue and tag the issue number you are addressing in the Pull Request</li> <li>Add one of the leads as a reviewer for the PR</li> </ul>"},{"location":"logging/log_details/","title":"Log Details","text":""},{"location":"logging/log_details/#federated-learning","title":"Federated Learning","text":"<p>Logs aggregated model metrics (loss and accuracy) and updates from clients to track the overall progress and performance of the federated learning process. Additionally, logs include training loss and accuracy from individual clients. Also logs communication events between the server and clients to monitor interactions and data exchange.</p>"},{"location":"logging/logging/","title":"Logging Documentation","text":"<p>This document provides a detailed overview of what is being logged in the Sonar setup. </p>"},{"location":"logging/logging/#overview","title":"Overview","text":"<p>This documentation aims to provide transparency on the logging mechanisms implemented in the Sonar project. It includes information on the types of data being logged, their sources, formats, and purposes.</p>"},{"location":"logging/logging/#logging-types","title":"Logging Types","text":"<ul> <li>DEBUG: Detailed information, typically of interest only when diagnosing problems.</li> <li>INFO: Confirmation that things are working as expected.</li> <li>Tensorboard logging: Logging specific metrics, images, and other data to TensorBoard for visualization and analysis.<ul> <li>Console Logging: Logs a message to the console.</li> <li>Scalar Logging: Logs scalar values to TensorBoard for tracking metrics(loss, accuracy) over time.</li> <li>Image Logging: Logs images to both a file and TensorBoard for visual analysis.</li> </ul> </li> </ul> <p>The tensorboard logs can be viewed by running tensorboard as follows: <code>tensorboard --logdir=expt_dump/ --host 0.0.0.0</code>. Assuming <code>expt_dump</code> is the folder where the experiment logs are stored.</p>"},{"location":"logging/logging/#log-sources","title":"Log Sources","text":"Component/Module Data Logged Log Level Format Storage Location Frequency/Trigger Model Training (FL) Aggregated model metrics, client updates INFO, DEBUG Plain text <code>./expt_dump/&lt;experiment_name&gt;/logs/client_&lt;client_index&gt;/summary.txt</code> On every FL round"},{"location":"tasks/image-classification/","title":"Image Classification","text":""},{"location":"tasks/object-detection/","title":"Object Detection","text":""},{"location":"tasks/object-detection/#overview","title":"Overview","text":"<p>Our environment supports object detection and classification tasks. We support an implementation of YOLOv3 objection detection model using the Pascal VOC dataset. The YOLOv3 (You Only Look Once, version 3) model is a state-of-the-art object detection algorithm known for its speed and accuracy. It performs both object detection and classification in a single forward pass through the network, making it highly efficient. In this project, we adapt YOLOv3 to work in a decentralized machine learning setup, which allows multiple users to train a shared model while keeping their data localized.</p>"},{"location":"tasks/object-detection/#credit","title":"Credit","text":"<p>The implementation of YOLOv3 in this project is based on the GeeksforGeeks YOLOv3 tutorial. Special thanks to the authors for providing a detailed guide that served as the foundation for this work.</p>"},{"location":"tasks/object-detection/#dataset-preparation","title":"Dataset Preparation","text":"<p>We use the Pascal VOC dataset, a popular benchmark in object detection tasks. Follow the steps below to download and prepare the dataset: 1) Download Pascal VOC Data:</p> <ul> <li>Visit the Pascal VOC Dataset page.</li> <li>Download the VOC 2012 dataset</li> </ul> <p>2) Extract and Structure the Dataset:</p> <ul> <li>Extract the downloaded dataset into a directory of your choice.</li> <li>Ensure the directory structure is as follows: <pre><code>VOCdevkit/\n  VOC2012/\n    Annotations/\n    ImageSets/\n    JPEGImages/\n    ...\n</code></pre></li> </ul> <p>3) (Optional) Split the data * You can split the data according to your desired distribution by labeling a text file with the image names. By default, we will use <code>train.txt</code> and <code>val.txt</code> in <code>VOC2012/Annotations/ImageSets/Main/</code></p>"},{"location":"tasks/object-detection/#configure-the-training","title":"Configure the Training","text":"<p>To set up the training environment, follow these instructions:</p> <p>1) Install Dependencies: If you haven't already, run <code>pip install -r requirements.txt</code> 2) Configure the system settings. In <code>src/configs/sys_config.py</code>, create a system config object such as the example below, with your desired settings. <pre><code>object_detection_system_config = {\n    \"num_users\": 3, \n    \"dset\": \"pascal\",\n    \"dump_dir\": \"./expt_dump/\", # the path to place the results\n    \"dpath\": \"./datasets/pascal/VOCdevkit/VOC2012/\", # the the location of the dataset\n    # node_0 is a server currently\n    # The device_ids dictionary depicts the GPUs on which the nodes reside.\n    # For a single-GPU environment, the config will look as follows (as it follows a 0-based indexing):\n    \"device_ids\": {\"node_0\": [0], \"node_1\": [0], \"node_2\": [0], \"node_3\": [0]},\n    \"samples_per_user\": 100, \n    \"train_label_distribution\": \"iid\",\n    \"test_label_distribution\": \"iid\",\n    \"folder_deletion_signal_path\":\"./expt_dump/folder_deletion.signal\"\n}\n</code></pre> 3) Configure the algorithm setting. In <code>src/configs/algo_config.py</code>, create an algo config object such as the example below, with your desired algorithm. <pre><code>fedavg_object_detect = {\n    \"algo\": \"fedavg\", # choose any algorithm we support\n    \"exp_id\": \"test\",\n    # Learning setup\n    \"epochs\": 50,\n    \"model\": \"yolo\",\n    \"model_lr\": 1e-5,\n    \"batch_size\": 8,\n}\n</code></pre> 4) Initiate Training: <code>mpirun -n 4 python3 main.py</code></p> <p>Note: the <code>-n</code> flag should be followed by (number of desired users+ 1), for the server node</p> <p>The training will proceed across the users as configured. Monitor printed or saved logs to track progress.</p> <p>Your result will be written into the <code>dump_dir</code> path specified in <code>sys_config.py</code></p>"},{"location":"tasks/object-detection/#additional-notes","title":"Additional Notes","text":"<ul> <li>Ensure that the setup is correctly configured to avoid issues with client-server communication.</li> <li>If you encounter any issues or have suggestions, please open an issue on our GitHub repository.</li> </ul>"},{"location":"tasks/text-classification/","title":"Text Classification","text":"<p>NOTE:  This task is a work in progress. We are currently running experiments to ensure support for Text Classification. It is currently not integrated properly with the system yet.</p>"},{"location":"tasks/text-classification/#overview","title":"Overview","text":"<p>Our environment supports text classification tasks using Long Short-Term Memory (LSTM) networks. We provide an implementation of an LSTM-based text classification model using the AGNews dataset. The LSTM model is a type of recurrent neural network (RNN) that is particularly effective for sequential data such as text. In this project, we adapt an LSTM network to classify news articles into one of four categories: World, Sports, Business, and Sci/Tech. The implementation is designed to handle decentralized machine learning scenarios, where multiple users can train a shared model while keeping their data localized.</p>"},{"location":"tasks/text-classification/#credit","title":"Credit","text":"<p>The AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu). It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015). This dataset is based on the AGNews Dataset. </p>"},{"location":"tasks/text-classification/#dataset-preparation","title":"Dataset Preparation","text":"<p>We use the AGNews dataset, a popular benchmark in text classification tasks. Follow the steps below to download and prepare the dataset: 1) Download AGNews Data:</p> <ul> <li>Visit the HuggingFace AGNews Dataset page to learn more about the dataset. </li> <li>Download the dataset and extract it to your working directory. If you're using HuggingFace, it can be loaded in using the following code: <code>dataset = datasets.load_dataset('ag_news')</code></li> </ul> <p>2) Construct the Vocabulary</p> <p>You must construct the vocabulary and preprocess the data by tokenizing the text and padding sequences to a uniform length. Here is the recommended script for preprocessing. <pre><code># Construct vocabulary from the dataset\nwords = Counter()\n\nfor example in dataset['train']['text']:\n    processed_text = example.lower().translate(str.maketrans('', '', string.punctuation))\n    for word in word_tokenize(processed_text):\n        words[word] += 1\n\nvocab = set(['&lt;unk&gt;', '&lt;bos&gt;', '&lt;eos&gt;', '&lt;pad&gt;'])\ncounter_threshold = 25\n\nfor char, cnt in words.items():\n    if cnt &gt; counter_threshold:\n        vocab.add(char)\n\nword2ind = {char: i for i, char in enumerate(vocab)}\nind2word = {i: char for char, i in word2ind.items()}\n</code></pre></p>"},{"location":"tasks/text-classification/#configure-the-training","title":"Configure the Training","text":"<p>To set up the training environment, follow these instructions: 1) Install Dependencies: If you haven't already, run <code>pip install -r requirements.txt</code>. 2) Configure the system settings. In <code>src/configs/sys_config.py</code>, create a system config object such as the example below, with your desired settings. <pre><code>text_classification_system_config = {\n    \"num_users\": 3, \n    \"dset\": \"agnews\",\n    \"dump_dir\": \"./expt_dump/\", # the path to place the results\n    \"dpath\": \"./datasets/agnews/\", # the location of the dataset\n    \"device_ids\": {\"node_0\": [0], \"node_1\": [0], \"node_2\": [0], \"node_3\": [0]},\n    \"samples_per_user\": 1000, \n    \"train_label_distribution\": \"iid\",\n    \"test_label_distribution\": \"iid\",\n    \"folder_deletion_signal_path\":\"./expt_dump/folder_deletion.signal\"\n}\n</code></pre> 3) Configure the algorithm settings. In <code>src/configs/algo_config.py</code>, create an algo config object such as the example below, with your desired algorithm. <pre><code>fedavg_text_classify = {\n    \"algo\": \"fedavg\", # choose any algorithm we support\n    \"exp_id\": \"test\",\n    # Learning setup\n    \"epochs\": 10,\n    \"model\": \"lstm\",\n    \"model_lr\": 1e-3,\n    \"batch_size\": 64,\n}\n</code></pre> 4) Initiate Training: <code>mpirun -n 4 python3 main.py</code></p> <p>Note: the <code>-n</code> flag should be followed by (number of desired users + 1), for the server node.</p> <p>The training will proceed across the users as configured. Monitor printed or saved logs to track progress.</p> <p>Your result will be written into the <code>dump_dir</code> path specified in <code>sys_config.py</code>.</p>"},{"location":"tasks/text-classification/#additional-notes","title":"Additional Notes","text":"<ul> <li>Ensure that the setup is correctly configured to avoid issues with client-server communication.</li> <li>If you encounter any issues or have suggestions, please open an issue on our GitHub repository.</li> </ul>"}]}